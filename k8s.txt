
---------------------------pv  pvc-------------------------------------------

删除pv  kubectl patch pv xxx -p '{"metadata":{"finalizers":null}}'

-----------------------------------------------------------------------------

---------------------------  pod -------------------------------------------
kubectl delete pod <name> --grace-period=0 --force -n <namespace>
----------------------------------------------------------------------------

-----------------------------------------------------------------------------

删除所有容器 docker rm $(docker ps -aq)

-----------------------------k8s 节点操作----------------------------------------

查看k8s的api资源类型 kubectl api-resources

查看k8s资源对象拥有的字段 kubectl explain <资源名对象名>

给节点添加标签 kubectl label nodes 10.18.127.4 node=kube-node

给节点修改标签 kubectl label nodes 10.18.127.4 node=kube-node --overwrite

删除节点标签 kubectl label nodes 10.18.127.4 node-

pod 强制删除(确定控制器被删除) kubectl -n <namespace> delete pod <pod-name> --force --grace-period=0

修复节点调度  kubectl patch node 10.18.127.4 -p '{"spec":{"unschedulable":false}}'

修改节点的ROLE(node->master) kubectl label node 10.18.127.3 --overwrite kubernetes.io/role=master

job扩容 kubectl scale  --replicas=$N jobs/myjob

statefulsets扩容 kubectl scale statefulset <sts> --replicas=4

滚动更新RC kubectl rolling-update my-nginx --image=nginx:1.9.1

更新sts镜像(statefulset) kubectl set image statefulset web nginx=nginx-slim:0.7

---------------------------------------------------------------------------------

查看pod日志  kubectl log mysql-77b495b4fd-pjw2w


--------------------------------docker用户---------------------------------------
1、新建用户组docker之前，查看用户组中有没有docker组，如果已经存在，则不需要再进行创建

sudo cat /etc/group | grep docker

2、创建docker分组，并将相应的用户添加到这个分组里面（999为组id，可以不指定）

sudo groupadd -g  docker

sudo usermod -aG dockerroot test

sudo usermod -aG docker test

3、检查一下创建是否创建成功

cat /etc/group | grep test

4、退出当前用户登陆状态，然后重新登录，以便让权限生效,或重启docker-daemon

sudo systemctl restart docker

5、确认你可以直接运行docker命令，执行docker命令

docker  info

通过命令 newgrp docker 让其生效

------------------------------查看docker进程调用关系-----------------------------
containerd的目录：/var/run/docker/containerd/containerd.toml

debug 保存目录：/var/run/docker/containerd/containerd-debug.sock

docker run -id ubuntu bash

ps fxa|grep docker -A 1



---------------------------------------------------------------------------------

---------------------------docker开启2375端口，提供外部访问docker----------------------

编辑docker文件：/etc/systemd/system/docker.service 或者 /usr/lib/systemd/system/docker.service

vim /etc/systemd/system/docker.service

修改ExecStart为下面内容
ExecStart=/opt/kube/bin/dockerd  -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock

重新加载docker配置
//1.加载docker守护进程
systemctl daemon-reload
//2.重启docker
systemctl restart docker

测试端口：
curl 127.0.0.1:2375/version

得到类似输出：
{"Platform":{"Name":"Docker Engine - Community"},"Components":[{"Name":"Engine","Version":"18.09.2","Details":{"ApiVersion":"1.39","Arch":"amd64","BuildTime"
:"2019-02-10T04:20:28.000000000+00:00","Experimental":"false","GitCommit":"6247962","GoVersion":"go1.10.6","KernelVersion":"3.10.0-957.27.2.el7.x86_64","MinAPIVersion":"1.12","Os":"linux"}}],"Version":"18.09.2","ApiVersion":"1.39","MinAPIVersion":"1.12","GitCommit":"6247962","GoVersion":"go1.10.6","Os":"linux","Arch":"amd64","KernelVersion":"3.10.0-957.27.2.el7.x86_64","BuildTime":"2019-02-10T04:20:28.000000000+00:00"}

# v1.39 docker API version
#请求dockerd创建容器，但由于dockerd在本地找不到相应的image，于是返回失败
curl 127.0.0.1:2375/v1.39/containers/create  -X POST -H "Content-Type: application/json" -d '{"Image": "hello-world"}'
{"message":"No such image: hello-world:latest"}

#请求dockerd去找registry服务器拿image
#这里的http应答中的内容包含了很多跟进度条相关的内容
curl '127.0.0.1:2375/v1.39/images/create?fromImage=hello-world&tag=latest' -X POST

/etc/resolv.conf 修改dns服务器

---------------------------------------------------------------------------------------

----------------------------------scratch镜像本地构建----------------------------------------

 tar cv --files-from /dev/null | docker import - scratch
 
---------------------------------------------------------------------------------------------

-------------------------------Docker无法启动 error initializing graphdriver: driver not supported--------------------------------
systemctl status docker

# Error starting daemon: error initializing graphdriver: driver not supported

解决办法:
在/etc/docker/daemon.json目录下，编辑如下内容：
{
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}

再次启动
systemctl start docker

----------------------------------------------------------------------------------------------------------------------------------

--------------------------------私有仓库-----------------------------------------

查看私有仓库镜像 curl http://10.18.127.1:5000/v2/_catalog

查看私有仓库镜像标签 curl http://10.18.127.1:5000/v2/busybox/tags/list

私有仓库客户端添加HTTP 请求
/etc/docker/daemon.json
{"insecure-registries": ["10.18.127.2:5000"],}

systemctl restart docker

journalctl -xe  查看出错原因
查看配置文件(一般是配置文件冲突)
vim /etc/systemd/system/docker.service 


删除私有仓库镜像（仅元数据）  curl -v -X DELETE 
http://10.18.127.1:5000/v2/tensorflow-1.13.1-notebook-cpu/manifests/sha256:5b32df24b3bb2b8692ed1e73fc240ac702d479d1e9cd141024099ec018184512

运行私有镜像(可删除)

docker run -itd \
 -v /data/registry:/var/lib/registry \
 -p 5000:5000 \
 --restart=always \
 -e REGISTRY_STORAGE_DELETE_ENABLED=true \
 --name registry registry:2.7


删除私有镜像（垃圾回收遗留镜像数据）
# docker exec -it registry  sh
# cd /var/lib/
# du -sh registry/
# registry garbage-collect /etc/docker/registry/config.yml
# du -sh registry/
# exit

------------------------------------------------------------------------------------


----------------------------------------------------harbor 私有仓库------------------------------------------------------

安装harbor过程：

1、安装底层需求：
 python应该是2.7或更高版本
 Docker引擎应为1.10或更高版本
 Docker Compose需要为1.6.0或更高版本

 docker-compose:

 curl -L https://github.com/docker/compose/releases/download/1.9.0/docker-compose-`uname -s`-`uname -m`

 >/usr/local/bin/docker-compose

 2、harbor安装：harbor官方地址：https://github.com/vmware/harbor/releases
 
 2.1 解压软件包： tar -xvf harbor-offline-installer-<version>.tgz https://github.com/vmware/harbor/releases/download/v1.2.0/harbor-offline-installer-v1.2.0.tgz

 2.2 配置 : harbor.cfg
  a、必选参数 、 hostname：目标的主机名或者完全限定域名 ： ui_url_protocol：http或https。默认为 。 http db_password：用于 ： db_auth的MySQL数据库的根密码。更改此密码进行任何生产用途 数 max_job_workers：（默认值为 ： 3）作业服务中的复制工作人员的最大数量。对于每个映像复制作业， ） 工作人员将存储库的所有标签同步到远程目标。增加此数字允许系统中更多的并发复制作业。但是，由于每个工 工作人员都会消耗一定数量的网络 作 / CPU/IO资源，请根据主机的硬件资源，仔细选择该属性的值 资 customize_crt：（： on或off。默认为 。 on）当此属性打开时， ） prepare脚本将为注册表的令牌的生成 脚 /验证创 验 建私钥和根证书 建 ssl_cert：SSL证书的路径，仅当协议设置为 证 https时才应用 时 ssl_cert_key：SSL密钥的路径，仅当协议设置为 密 https时才应用 时 secretkey_path：用于在复制策略中加密或解密远程注册表的密码的密钥路径

 2.3 创建 https 证书以及配置相关目录权限
 mkdir -p /data/cert
 cd /data/cert

 openssl genrsa -des3 -out server.key 2048
 openssl req -new -key server.key -out server.csr
 cp server.key server.key.org
 openssl rsa -in server.key.org -out server.key
 openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt

 2.4 运行脚本进行安装
 ./install.sh

 2.5 访问测试
 https://reg.yourdomain.com的管理员门户（将 reg.yourdomain.com更改为您的主机名 更 harbor.cfg）。请注意，默认管理员用户名/密码为 admin /Harbor12345

 2.6 上传镜像进行上传测试

   a、指定镜像仓库地址：
   vim /etc/docker/daemon.json
    {"insecure-registries": ["serverip"] }
   b、下载测试镜像 ： docker pull hello-world
   c、给镜像重新打标签 ： docker tag hello-world serverip/library/hello-world:latest 
   d、登录进行上传 ： docker login serverip

 2.7 其它, Docker客户端下载测试 
  a、指定镜像仓库地址 : 
  vim/etc/docker/daemon.json {"insecure-registries": ["serverip"] }

  b、下载测试镜像 : docker pull serverip/library/hello-world:latest



重启docker后，harbor服务挂掉重启：

cd /usr/local/harbor/

看到docker-compose.yml可知使用了docker-compose，可以使用后台启动的方式来实现harbor的开机启动功能

停止容器
docker-compose stop

后台启动容器
docker-compose up -d

------------------------------------------------------------------------------------------------------------------------

------------------------nfs----------------------------------

给共享目录添加权限：
    chown -R nfsnobody.nfsnobody /sharedir/
把NFS共享目录赋予 NFS默认用户nfsnobody用户和用户组权限，如不设置，会导致NFS客户端无法在挂载好的共享目录中写入数据

nfs挂载  mount -t nfs 10.18.129.161:/mnt/xfs /mnt/xfs

nfs客户端卸载挂载目录：
umount /mnt/xfs

客户端查看目录挂载情况：
df /mnt/xfs

nfs服务端卸载：
systemctl stop nfs-server
systemctl stop rpcbind.service / systemctl stop rpcbind.socket


nfs服务端开启nfs服务：
systemctl start nfs.service
systemctl start rpcbind.service


nfs服务端nfs服务状态查看：
systemctl status nfs.service
systemctl status rpcbind.service

NFS服务端配置：
vim /etc/exports
<输出目录> [客户端1 选项（访问权限,用户映射,其他）] [客户端2 选项（访问权限,用户映射,其他）]
/mnt/xfs         10.18.127.0/8(rw)

查看nfs服务器配置：
/mnt/xfs        10.18.127.0/8

服务器端查询nfs共享状态：
showmount -e
# Export list for master:
# /mnt/xfs 10.18.127.0/8

NFS权限设定： TODO

--------------------------------------------------------------


--------------------------Python3 源码安装-------------------------------------
 
yum install -y zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make libffi-devel

Ubuntu:
apt install -y build-essential libssl-dev zlib1g-dev \
libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \
libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev

wget https://www.python.org/ftp/python/3.7.2/Python-3.7.2.tar.xz

tar -xvf Python-3.7.2.tar.xz

#tar -zxvf Python-3.7.2.tar.gz

cd Python-3.7.2

#编译安装
./configure prefix=/usr/local/python3
make && make install

#安装后 /usr/local目录下会有python3


# 添加软链接
ln -sf /usr/local/python3/bin/python3.7 /usr/bin/python3

#ln -s /usr/local/python3/bin/python3.7 /usr/bin/python3

# 验证
python -V


安装第三方库
python3 -m pip -V
python3 -m pip install -y Pillow

-------------------------------------------------------------------------------


--------------------------Python3 非root用户 源码安装-------------------------------------

wget https://www.python.org/ftp/python/3.7.2/Python-3.7.2.tar.xz
tar -xzf Python-3.7.2.tar.gz
cd Python-3.7.2
mkdir -p /home/xxx/software/python3.7
# 编译安装
./configure --prefix="/home/xxx/software/python3.7"
make
make install


vim ~/.bashrc
#添加该语句
alias python3=/home/xxx/software/python3.6/bin/python3.6
alias pip3=/home/xxx/software/python3.6/bin/pip3.6

source .bashrc

python3 -V
pip3 -V

---------------------------------------------------------------------------------------------


-------------------------------时间同步方式------------------------------------

docker run -it --cap-add SYS_TIME pipelineai/kubeflow-notebook-cpu-1.13.1:2.0.2 /bin/bash
|| docker run -it --privileged pipelineai/kubeflow-notebook-cpu-1.13.1:2.0.2 /bin/bash

apt-get -y update && apt-get install -y tzdata

yum install -y ntp

ntpdate ntp1.aliyun.com

ntpdate cn.pool.ntp.org

docker commit -a "wzy" -m "my apache" a404c6c174a2  mymysql:v1
-------------------------------------------------------------------------------


-----------------------------kubeflow notebook 镜像构建 Dockerfile------------------------

FROM pipelineai/kubeflow-notebook-cpu-1.13.1-timezone:2.0.2
ENV NB_PREFIX /
CMD ["sh","-c", "jupyter notebook --notebook-dir=/home/jovyan --ip=0.0.0.0 --no-browser --allow-root --port=8888 --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.allow_origin='*' --NotebookApp.base_url=${NB_PREFIX}"]

-------------------------------------------------------------------------------

-------------------------------ssh 免密登录-------------------------------------------
生成密钥对： ssh-keygen -t rsa -P ""
拷贝本机公钥到另一个节点authorized keys文件中 ssh-copy-id root@10.18.127.4
--------------------------------------------------------------------------------------

----------------------------------centos安装软件包------------------------------------
针对错误消息中的每个依赖包,执行命令repoquery --nvr --whatprovides   <依赖文件>,安装输出包, such as
repoquery --nvr --whatprovides libXss.so.1
### libXScrnSaver-1.2.2-6.1.el7
--------------------------------------------------------------------------------------

--------------------------------------打包/解压----------------------------------------------

gzip压缩命令:     tar -zcvf 归档路径 被打包路径.  (gun-zip压缩,后缀一般是.tar.gz)

bzip2压缩命令:   tar -jcvf  归档路径 被打包路径.  (bzip2压缩,后缀一般是.tar.bz2)

tar -jcvf  kubeasz.tar.bz2 kubeasz/


tar -xvf 归档文件路径   ---可拆tar/gzip/bzip2格式的包

gzip压缩命令：   tar -zxvf 归档文件路径   ---只可拆gzip格式的包

bzip2解压命令：  tar -jxvf 归档文件路径    ---只可拆bzip2格式的包
---------------------------------------------------------------------------------------------

-------------------------------------gcc 静态编译---------------------------------------------
gcc --static -o hello hello.c 报错:
/usr/bin/ld: cannot find -lc

静态编译时需要将所有的.a库链接到可执行文件中，所以需要libc静态库文件，在系统找查找glibc-static提示没有库文件
yum install -y glibc-static
----------------------------------------------------------------------------------------------
----------------------------------------查看firewall防火墙------------------------------------
# 查看firewall开放的端口
firewall-cmd --zone=public --list-ports

# 防火墙重启
systemctl restart firewalld

# 打开某个端口
firewall-cmd --zone=public --add-port=2375/tcp --permanent
firewall-cmd --reload

# 删除某个端口
firewall-cmd --zone=public --remove-port=2375/tcp --permanent
firewall-cmd --reload

# 关闭防火墙
systemctl stop firewalld

# 查看应用占用的端口
ss -tunlp | grep teamviewerd

----------------------------------------------------------------------------------------------


--------------------------------------iptables 管理端口-------------------------------------------------

ubuntu下

安装netfilter

sudo apt install -y iptables-persistent

其中，参数：

–A：参数就看成是添加一条规则
–p：指定协议，我们常用的tcp 协议。当然也有udp，例如53端口的DNS
–dport：就是目标端口，当数据从外部进入服务器为目标端口
–sport：数据从服务器出去，则为数据源端口使用
–j：就是指定是 ACCEPT接收，或者 DROP 拒绝
–s：指定IP

# 查看端口情况
iptables -L

# 开启7890端口

sudo iptables -I INPUT -p tcp --dport 7890 -j ACCEPT

sudo iptables -A INPUT -p tcp --dport 7890 -j ACCEPT

# 关闭端口

sudo iptables -A INPUT -p tcp --dport 7890 -j DROP

关闭IP访问：

$ iptables -A INPUT -p tcp -s 192.168.1.2 -j DROP

# 保存配置
sudo netfilter-persistent save

# reload重载更新
sudo netfilter-persistent reload

sudo iptables -L -n --line-number # 显示规则行号

sudo iptables -D INPUT 3 # 删除INPUT的第三条已添加规则，这里的3代表第几行规则

# 屏蔽客户端TCP报文中标志位是ACK的包

sudo iptables -I INPUT -s 192.168.12.37 -p tcp --tcp-flag ACK ACK -j DROP

# 查看iptables开启状态

sudo systemctl status iptables

添加 iptables 限制后， tcpdump 是否能抓到包 ，这要看添加的 iptables 限制条件：

如果添加的是 INPUT 规则，则可以抓得到包
如果添加的是 OUTPUT 规则，则抓不到包
网络包进入主机后的顺序如下：

进来的顺序 Wire -> NIC -> tcpdump -> netfilter/iptables
出去的顺序 iptables -> tcpdump -> NIC -> Wire

效果大致如下：

Chain INPUT (policy ACCEPT)
target     prot opt source               destination         
ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:7890

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination         

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination


-------------------------------------------------------------------------------------------------------

-----------------------------------------------tcpdump 使用--------------------------------------
tcpdump使用 - 选项类
选项                    示例                       说明
-i               tcpdump -i eno1              指定网络接口，默认是0号接口（如eth0），any表示所有接口
-nn              tcpdump -nn                  不解析IP地址和端口号的名称
-c               tcpdump -c 5                 限制要抓取的网络包的个数
-w               tcpdump -w file.pcap         保存到文件中，文件名通常以.pcap为后缀


tcpdump 过滤表达式使用

选项                             示例                                        说明
host、 src host、dst host   tcpdump -nn host 192.168.1.100                  主机过滤
port、 src port、dst port   tcpdump -nn port 80                             端口过滤
ip ip6、arp、tcp、udp、icmp   tcpdump -nn tcp                                协议过滤
and、or、not                 tcpdump -nn host 192.168.1.100 and port 80     逻辑表达式
tcp[tcpflages]             tcpdump -nn "tc[tcpflags] & tcp-syn!=0"        特定状态的TCP包

使用示例:icmp协议 ，将抓取到的数据包保存到 ping.pcap文件中，另起一个终端执行 $ping -I eno0 -c 3 192.168.1.100

tcpdump -i eno1 icmp and host 192.168.1.100 -w ping.pcap

保存好的ping.pcap可以使用Wireshark打开分析

ep2：

tcpdump -i eno1 tcp and host 10.208.16.31 and port 8080 -w tcp_sec_ack_timeout.pcap

$curl -i 10.208.16.31:8080

ping命令
# -I eno0 表示指定从eno1网口出去
# -c 3 表示发出3个icmp数据包

ping -I eno0 -c 3 10.208.16.31

--------------------------------------------------------------------------------------------------

------------------------------------------grep筛选输出标题-----------------------------------------------
将第6列进行排序，最大的数排前面，只想看前10条数据
ep : 将实际内存消耗最大的10个进程显示出来的命令
ps auxw|head -1;ps auxw|sort -rn -k6|head -10

-------------------------------------------------------------------------------------------------

-------------------------------ufw 防火墙-------------------------------------------------
1. 安装  ufw 

sudo apt-get install ufw

2. 启用/禁用 ufw

启用ufw命令：
sudo ufw enable (注意如果已经打开ufw，再次开启可能会终止存在的ssh连接)

禁用ufw命令：
sudo ufw disable

查看防火墙当前的状态：

sudo ufw status
Status: active

To                         Action      From
--                         ------      ----
7890                       ALLOW IN    Anywhere                  
22                         ALLOW IN    Anywhere                  
7890 (v6)                  ALLOW IN    Anywhere (v6)             
22 (v6)                    ALLOW IN    Anywhere (v6)  


允许/拒绝连接

允许指定端口连接
sudo ufw allow 21

sudo ufw allow 6000:6007/tcp
sudo ufw allow 6000:6007/udp

sudo ufw allow from 15.15.15.51

允许ip网段的连接

允许ip段15.15.15.1到15.15.15.254的所有连接

sudo ufw allow from 15.15.15.0/24

禁止80端口的访问
sudo ufw deny 80 

禁止来自某个ip访问
sudo ufw deny from 15.15.15.51

通过规则删除
首先查看所有规则的规则号：

sudo ufw status numbered

     To                         Action      From
     --                         ------      ----
[ 1] 7890                       ALLOW IN    Anywhere                  
[ 2] 22                         ALLOW IN    Anywhere                  
[ 3] 7890 (v6)                  ALLOW IN    Anywhere (v6)             
[ 4] 22 (v6)                    ALLOW IN    Anywhere (v6) 


然后delete规则号即可

sudo ufw delete 1
sudo ufw delete 3 


重置防火墙规则

sudo ufw reset 
该命令将禁用ufw，并删除所有已经定义的规则，不过默认该命令会对以设置的规则备份

------------------------------------------------------------------------------------------

---------------------------yum 安装rpm解决依赖库------------------------
yum --nogpgcheck localinstall packagename.arch.rpm

rpm 卸载

首先查找安装：
rpm -qa | grep -i 软件名

然后卸载：
sudo rpm -e -- 包名


dpkg解决依赖库问题
## 添加路径很重要
sudo apt-get install ./packagename.arch.deb

dpkg查看已安装

sudo dpkg -l | grep 包名

dpkg卸载

sudo dpkg -r 软件名

sudo dpkg --purge 软件名 (卸载并删除配置)

dpkg -get-selections | grep 软件名

sudo apt-get remove -purge 软件名


-------------------------------------------------------------------------------------------------

---------------------------------hdfs ---------------------------------------------------------
hdfs 退出安全模式  hdfs dfsadmin -safemode leave
-----------------------------------------------------------------------------------------------

-----------------------------------网桥---------------------------------------------------
# 停止docker service
systemctl stop docker
 
#关闭docker0网卡(如果没有ifconfig，则需要先安装(yum install net-tools))
ifconfig docker0 down
 
#删除docker0网卡(如果没有brctl，则需要先安装(yum install bridge-utils))
brctl delbr docker0
------------------------------------------------------------------------------------------

-----------------------------------------时间同步------------------------------------------
# 如果没有date，则需要先安装date命令
yum install -y coreutils

# 安装ntpdate 
yum install -y ntpdate(NTP network time protocol)

# 时间同步
ntpdate ntp1.aliyun.com
------------------------------------------------------------------------------------------
-----------------------------------------sed----------------------------------------------
截取指定行之间的内容到新的文件中
sed -n '开始行数,结束行数p' 待截取文件 >> 保存的
------------------------------------------------------------------------------------------



-----------------------------------------编译源代码指定安装路径---------------------------------------
一般编译源代码三部曲
 ./configure
 make
 make install
 这种默认安装路径的话，会把可执行文件拷贝到/usr/local/bin，如果没有sudo权限的话是会失败的，有两种方法指定安装路径。
  一种是在./configure的时候指定路径
  ./configure --prefix=安装路径
  一种是make install的时候指定路径
  make install DESTDIR=安装路径
----------------------------------------------------------------------------------------------------


----------------------------------------CentOS7 源码编译QEMU 需要安装的依赖-------------------------------------------------------------
yum install -y epel-release
yum install centos-release-scl
yum install -y devtoolset-8-gcc*
yum install -y libaio-devel libcap-devel libiscsi-devel
yum install python36-devel -y
yum install -y ninja-build
yum install -y glib pixman-devel
yum install -y cmake3 zlib-devel
----------------------------------------------------------------------------------------------------



-------------------------------------vim tab转为space---------------------------------------------------------

set tabstop=4
set expandtab 用space替代tab的输入
%ret!4
-------------------------------------------------------------------------------------------------------------------------------------------------------------


--------------------------------------jupyter notebook------------------------------------------------------------------------------------------------

1.jupyter notebook --generate-config

此时系统会生成 ~/.jupyter/这个隐藏文件夹，且文件夹中生成了jupyter的配置文件jupyter_notebook_config.py


2、生成jupyter notebook 启动参数文件并修改参数
~$ jupyter notebook --generate-config
~$ vi ~/.jupyter/jupyter_notebook_config.py

打开python3命令行,输入如下命令：
from notebook.auth import passwd
passwd()
# 此时需要输入两次密码（一次设置，一次确认），然后生成sha1的密文，拷贝下来。
# Enter password: ········
# Verify password: ········
# sha1:b11ba7ae862e:6eeb922ef6b770e43a1c90922ba341faaaaaaaa

打开jupyter_notebook_config.py，滚动到文件最后一行，加入下面的配置选项

输入passwd()后需要两次键入密码，之后会产生一段密文


3、jupyter_notebook_config.py添加如下参数后，并保存：

c.NotebookApp.ip = '*'  #外部IP地址客户端可以访问
# 密码 2.1中生成'sha1:xxxx"那一大串，复制时包括sha1 
c.NotebookApp.password = 'sha1:0d00e0994444:3aeecafab4d91260d42647df8df4c3859a3430a9' 
c.NotebookApp.notebook_dir = '/home/wzy/notebook-dir'  #本地notebook访问的目录
c.NotebookApp.open_browser = False   #jupyter notebook启用时不再本地默认打开浏览器
c.NotebookApp.port = 9999            #默认访问的端口是8888


4、直接启动notebook后台服务，并输出运行日志，之后就可以远程访问这个服务了
~$ jupyter notebook --allow-root >.jupyter/jupyter_notebook.log 2>&1 &
[1] 10395

---------------------------------------------------------------------------------------------------------------------------------------------


------------------------------------------conda 环境-------------------------------------------------------------------------------------------

1、查看python环境
conda info --env 
 可以看到所有python环境，前面有*代表当前环境

2、创建环境

conda create --name python35 python=3.5 代表创建一个python3.5的环境，把它命名为python35

3、管理和使用python环境

source activate python35 来激活并使用刚才创建的环境

conda deactivate 退出当前激活的环境
source deactivate 来退出当前激活的环境

4、 删除创建的环境 python35
conda remove -n python35 --all

----------------------------------------------------------------------------------------------------------------------------


---------------------------------jupyter notebook中添加conda环境------------------------------------------

1、首先安装ipykernel：conda install ipykernel

python3 -m ipykernel install --user --name fate-1.6.0 --display-name "fate-1.6.0"

卸载jupyter中的虚拟环境：

jupyter kernelspec remove pytorch


2、新环境中安装pytorch，在终端、pycharm中import torch成功，但在jupyter中失败：ModuleNotFoundError: No module named 'torch'


原因：在使用Jupyter Notebook的时候，加载的仍然是默认的Python Kernel。在终端的python编译器中输入代码，查看路径；在jupyter中输入代码查看路径，可以发现二者路径不一样。


import sys

sys.executable

解决：如上安装nb_conda_kernels包。

conda install nb_conda_kernels

解决：在新环境中安装ipykernel。
conda install -n 环境名称 ipykernel #直接指定环境安装ipykernel

查看所有的kernel
jupyter kernelspec list

python -m ipykernel install --user（非服务器可缺省） --name 环境名称 #写入jupyter notebook 的kernel


删除安装的kernel

jupyter kernelspec remove tensorflow28

---------------------------------------------------------------------------------------------------------------------------



----------------------挂载磁盘------------------------

查看目录大小 df -lh

查看所有磁盘 fdisk -l

查看磁盘文件系统 parted -l

根据查到的磁盘文件格式来格式化磁盘 mkfs -t ext3/ext4  /dev/sdb

把格式化之后的磁盘挂载到文件目录上
mount /dev/sdb /home

再次查看磁盘挂载目录情况
df -lh

上面配置完重启之后还是需要手动挂载一次，所以需要添加开机自动挂载磁盘
vim /etc/fstab

在末尾添加
/dev/sdb  /home  ext4 defaults 0 0


列参数说明：
第五列是dump备份设置。

当其值设置为1时，将允许dump备份程序备份；设置为0时，忽略备份操作；

第六列是fsck磁盘检查设置。

其值是一个顺序。当其值为0时，永远不检查；而 / 根目录分区永远都为1。其它分区从2开始，数字越小越先检查，如果两个分区的数字相同，则同时检查。

当修改完此文件并保存后，重启服务器生效。



------------------------------------------------------



----------------------------------------文本从windows拷贝到linux 执行出现错误---------------------------------------------
出现这种情况，通常都是在window下写好的shell脚本，拷贝到linux机器执行 才会出现

原因就是：

windows下每一行的结尾是\n\r，而在linux下文件的结尾是\n

这样window写编辑的shell脚本拷贝到linux下时

每一行的结尾就会多出来一个字符\r

这个\r字符会被显示为^M

所以就会出现错误提示" /bin/bash^M: bad interpreter: No such file or dire",

字面意思就是/bin/bash^M: 坏的解释器: 没有那个文件或目录

解决方法：

终端执行

sed -i 's/\r$//' filename  #flename即shell脚本文件名

这个命令会把以\r结束的字符换成空白
------------------------------------------------------------------------------------------------------------------------


------------------------------------------------GDB 远程调试-------------------------------------------
前期：准备好指定版本的 gcc (6.5.0) cmake(3.19.5) gdb(8.2.1) 源码安装

附上 Clion官网 的链接： https://blog.jetbrains.com/zh-hans/2019/04/17/ide/

远程运行：

1、Files | Settings | Build, Execution, Deployment | Deployment 部署(设置自动上传 Tools | Deployment | Automatic Upload)

设置远程服务器连接 Connection

Type: SFTP
SSH configuration: wzy@10.18.127.5:22
Root path: /home/wzy
Web Server URL: http://10.18.127.5


设置远程服务器连接 Mapping

Local path:C:\Users\wzy\CLionProjects\Cplusplus-func-pointer
Deployment path: cpp-code/Cplusplus-func-pointer (相对路径{相对 Root path})
Web path: /


2、Preferences | Build, Execution, Deployment | Toolchains 设置远程工具链信息

设置远程连接信息，CLion会自动监测CMake gcc g++ gdb的信息(注意手动指定，否则默认情况下只会填/bin/下的二进制文件)

Name: 127.5
Credentials: wzy@10.18.127.5:22
Cmake: cmake (usr/bin/cmake)
Make: (Detacted:/usr/bin/gmake)
C Compiler: /usr/local/bin/gcc
C++ Compiler: /usr/local/bin/g++
Debugger: Remote Host GDB(/usr/bin/gdb)

3、Preferences | Build, Execution, Deployment | CMake 设置远程构建信息

Name:debug-remote
Build type: Debug
Toolchain: 127.5 (刚才设置的工具链)

Cmake options: (-DCMAKE_BUILD_TYPE=Debug -DCMAKE_C_COMPILER=/usr/local/bin/gcc -DCMAKE_CXX_COMPILER=/usr/local/bin/g++) 默认

Build directory: cmake-build-debug-remote

Build options: - - j 6


使用gdb 进行 Debug 远程 调试

CLion Run/Debug Configurations设置

添加一个GDB远程调试的配置 GDB Remote Debug
设置远程访问参数（target remote args）： tcp:xx.xx.xx.xx:1234
设置远程路径和本地路径的映射（Path mappings）
远程调试连接成功后，像本地调试一样，可以设置断点，单步跟踪等
调试需要本地和远程的代码一致


两个方法设置远程配置
方法一、 远程gdbserver的启动  (打印中文会乱码)

远程调试依赖gdbserver的来支持，通过gdbserver的启动的程序，会先等待远程调试的连接，连接成功后再启动进程。

假设代码的根目录：/home/wzy/cpp-code/Cplusplus-func-pointer/,执行以下代码以后编译

cd /home/wzy/cpp-code/Cplusplus-func-pointer/cmake-build-debug

cmake .. -DCMAKE_BUILD_TYPE=Debug

make

gdbserver :1234 ./Cplusplus-func-pointer

注意：cmake的指定需要-DCMAKE_BUILD_TYPE=Debug来请获取调试


方法二 远程gdbserver的动态连接

gdbserver的还支持动态绑定到已经启动的进程

gdbserver :1234 --attach <PID>

------------------------------------------------------------------------------------------------------

------------------------------------源码安装gcc、cmake、gdb可能出现的一些错误以及解决方案-----------------------------------

安装cmake3.19.5 运行./bootstrap的时候出现如下提示:

/usr/soft/cmake-3.5.1/Bootstrap.cmk/cmake: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /usr/soft/cmake-3.5.1/Bootstrap.cmk/cmake)
---------------------------------------------
Error when bootstrapping CMake:

缺少GLIBCXX_3.4.21，或更高版本
为了核实版本问题：
# strings /usr/lib64/libstdc++.so.6|grep GLIBCXX

发现没有GLIBCXX_3.4.21 ，但是我刚已经安装过gcc 6.5.0
顺着gcc的安装路径，找到了新的libstdc++:

# strings /usr/local/lib64/libstdc++.so.6|grep GLIBCXX


接下来，把这份软链到正确的地方，就好了
cp /usr/local/lib64/libstdc++.so.6.0.21 /usr/lib64/
cd /usr/lib64/
rm -f libstdc++.so.6
ln -s libstdc++.so.6.0.21 libstdc++.so.6


至此再次运行
./bootstrap --prefix=/usr/local/cmake
make -j8
sudo make install

cmake安装完毕  设置软链接 这里略过

ep : ln -sf /usr/local/cmake/bin/... /usr/bin/cmake


安装 gdb 
1、 下载

2、 解压 tar -zxvf gdb-8.2.1.tar.gz , 首先安装 yum install texinfo 防止后面出现 makeinfo is missing on your system 错误


3、 配置 ./bootstrap prefix=/usr/local/gdb-8.2.1 或 ./configure prefix=/usr/local/gdb-8.2.1

4、编译

make -j8

5、安装

sudo make install

如果安装过程中出现错误，则首先 根据提示：

error: run `make distclean' and/or `rm ./config.cache' and start over

make distclean(类似make clean ，同时也将configure生成的配置文件也全部删除掉,包括Makefile) 
rm ./config.cache

最后 从步骤3 再开始走一遍

-----------------------------------------------------------------------------------------------------------------------


--------------------------------------------------Java 远程调试---------------------------------------------------------

IDEA 下 远程 jar 调试

首先在IDEA 开发完成后将 jar 打包完成，上传到服务器上

然后 在 Run/Debug Configurations | Remote JVM Debug

Debugger mode: Attach to remote JVM
Transport: Socket
Host: 10.18.127.5   Port:8000

Command line arguments for remote JVM:

(-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:8000) 自动生成

Use module classpath: Hip-conversion


服务器端启动：

java -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=0.0.0.0:8000 -jar Hip-conversion-1.0.jar

注意这里必须将地址写为 0.0.0.0 让客户端连接上去，不然 8000 端口只跟 127.0.0.1(服务器ip绑定：安全)

然后就可以端点调试运行了


-----------------------------------------------------------------------------------------------------------------------
-------------------------------------------Linux给用户添加 CAP_SYS_ADMIN 权限---------------------------------------------

Nvidia Nsight System 使用需要管理员或者用户具有CAP_SYS_ADMIN权限

Nvidia Nsight Compute 普通用户使用

echo 'options nvidia "NVreg_RestrictProfilingToAdminUsers=0"' | sudo tee -a /etc/modprobe.d/nvidia.conf
sudo update-initramfs -u 
sudo reboot

Ubuntu下给user1添加CAP_SYS_ADMIN权限

打开 /etc/security/capability.conf 配置文件 添加cap_sys_admin

$ cat /etc/security/capability.conf
cap_sys_admin   user1

添加pam_cap.so 到PAM. 注意 pam_cap.so 必须要添加在pam_rootok.so 前面

$ cat /etc/pam.d/su
#%PAM-1.0
auth        optional    pam_cap.so
auth        sufficient  pam_rootok.so
...
...

Example 

$ su user1

$ capsh --print

Current: = cap_sys_admin+i
Bounding set =cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,35,36
Securebits: 00/0x0/1'b0
 secure-noroot: no (unlocked)
 secure-no-suid-fixup: no (unlocked)
 secure-keep-caps: no (unlocked)
uid=1001(user1)
gid=1001(user1)
groups=1001(user1)

-------------------------------------------------------------------------------------------------------------------------
